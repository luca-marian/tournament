{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (4.36.2)\n",
      "Requirement already satisfied: filelock in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (0.15.0)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (0.4.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from transformers) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests->transformers) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests->transformers) (2023.11.17)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: spacy in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (3.7.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.1.8 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (8.2.2)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (6.4.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (4.66.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (2.31.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (2.5.3)\n",
      "Requirement already satisfied: jinja2 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (3.1.2)\n",
      "Requirement already satisfied: setuptools in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (68.2.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (23.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from spacy) (1.26.3)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.14.6)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.11.17)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from thinc<8.3.0,>=8.1.8->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (7.1.2)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages (from jinja2->spacy) (2.1.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/spacy/__init__.py\", line 6, in <module>\n",
      "    from .errors import setup_default_warnings\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/spacy/errors.py\", line 3, in <module>\n",
      "    from .compat import Literal\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/spacy/compat.py\", line 4, in <module>\n",
      "    from thinc.util import copy_array\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/thinc/__init__.py\", line 5, in <module>\n",
      "    from .config import registry\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/thinc/config.py\", line 5, in <module>\n",
      "    from .types import Decorator\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/thinc/types.py\", line 25, in <module>\n",
      "    from .compat import cupy, has_cupy\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/thinc/compat.py\", line 32, in <module>\n",
      "    import torch\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/__init__.py\", line 1504, in <module>\n",
      "    from . import masked\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/masked/__init__.py\", line 3, in <module>\n",
      "    from ._ops import (\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/masked/_ops.py\", line 11, in <module>\n",
      "    from torch._prims_common import corresponding_real_dtype\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/torch/_prims_common/__init__.py\", line 23, in <module>\n",
      "    import sympy\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/__init__.py\", line 74, in <module>\n",
      "    from .polys import (Poly, PurePoly, poly_from_expr, parallel_poly_from_expr,\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/polys/__init__.py\", line 123, in <module>\n",
      "    from .partfrac import apart, apart_list, assemble_partfrac_list\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/polys/partfrac.py\", line 15, in <module>\n",
      "    def apart(f, x=None, full=False, **options):\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/utilities/decorator.py\", line 76, in xthreaded\n",
      "    return threaded_factory(func, False)\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/utilities/decorator.py\", line 13, in threaded_factory\n",
      "    from sympy.matrices import MatrixBase\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/matrices/__init__.py\", line 7, in <module>\n",
      "    from .dense import (\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/matrices/dense.py\", line 14, in <module>\n",
      "    from .matrices import MatrixBase\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/matrices/matrices.py\", line 41, in <module>\n",
      "    from .eigen import (\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/matrices/eigen.py\", line 15, in <module>\n",
      "    from sympy.polys.matrices.eigen import dom_eigenvects, dom_eigenvects_to_sympy\n",
      "  File \"/Users/marianluca/anaconda3/envs/env_pytorch/lib/python3.9/site-packages/sympy/polys/matrices/eigen.py\", line 8, in <module>\n",
      "    from ..agca.extensions import FiniteExtension\n",
      "  File \"<frozen importlib._bootstrap>\", line 1007, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 986, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 680, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 846, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 941, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1040, in get_data\n",
      "KeyboardInterrupt\n",
      "Collecting en-core-web-lg==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_lg-3.7.1/en_core_web_lg-3.7.1-py3-none-any.whl (587.7 MB)\n",
      "\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.5/587.7 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:25\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Run if you run on colab\n",
    "%pip install transformers\n",
    "%pip install spacy\n",
    "!python -m spacy download en_core_web_sm\n",
    "!python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BertModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_mentions_and_hashtags(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Remove punctuation using the translation table\n",
    "    text_without_punct = text.translate(translator)\n",
    "\n",
    "    return text_without_punct\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_sentence = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False:\n",
    "            filtered_sentence.append(token.text)\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "\n",
    "def clean_text(text,to_lemmatize:bool = True):\n",
    "\n",
    "    # Standardize text\n",
    "    # text = standardize_accented_chars(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = remove_mentions_and_hashtags(text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = remove_numbers(text)\n",
    "\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "    if to_lemmatize:\n",
    "        text = lemmatize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_test, title: str = \"Confusion Matrix\"):\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "    print(f\"Reports for {title}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred,average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred,average='weighted')}\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred,average='weighted')}\")\n",
    "    print(f\"Metthew corr: {matthews_corrcoef(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    # sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "    # plt.title(title)\n",
    "    # plt.xlabel(\"Predicted\")\n",
    "    # plt.ylabel(\"True\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    \"\"\"Function to split dataset into train, val and test\"\"\"\n",
    "    # np.random.seed(112)\n",
    "    # df_train, df_val, df_test = np.split(\n",
    "    #     data.sample(frac=1, random_state=42),\n",
    "    #     [int(0.8 * len(data)), int(0.9 * len(data))],\n",
    "    # )\n",
    "\n",
    "    # # Print dimensions\n",
    "    # print(len(df_train), len(df_val), len(df_test))\n",
    "\n",
    "    # Split data into train (80%) and temp_test (20%)\n",
    "    df_train, temp_test = train_test_split(\n",
    "        data, test_size=0.2, random_state=42, stratify=data[\"target\"]\n",
    "    )\n",
    "\n",
    "    # Split temp_test into validation (50% of temp_test, 10% of total) and test (50% of temp_test, 10% of total)\n",
    "    df_val, df_test = train_test_split(\n",
    "        temp_test, test_size=0.5, random_state=42, stratify=temp_test[\"target\"]\n",
    "    )\n",
    "\n",
    "    # Print dimensions to verify\n",
    "    print(\n",
    "        f\"Train size: {len(df_train)}, Validation size: {len(df_val)}, Test size: {len(df_test)}\"\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "DATA_PATH = os.path.join(parent_directory, \"data\")\n",
    "DATA_TOURNAMENT = os.path.join(DATA_PATH, \"tournament_hints_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /Users/marianluca/Projects/HackingBigNumbers/Tournament/marian\n",
      "Parent Directory: /Users/marianluca/Projects/HackingBigNumbers/Tournament\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "DATA_PATH = os.path.join(parent_directory,\"data\")\n",
    "DATA_TOURNAMENT = os.path.join(DATA_PATH,\"tournament_hints_data.parquet\")\n",
    "\n",
    "print(\"Current Directory:\", current_directory)\n",
    "print(\"Parent Directory:\", parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commercial_name</th>\n",
       "      <th>business_tags</th>\n",
       "      <th>short_description</th>\n",
       "      <th>description</th>\n",
       "      <th>main_business_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>White Horse</td>\n",
       "      <td>Tile Manufacturing | European Aesthetics Ceram...</td>\n",
       "      <td>White Horse is highly regarded as a tile trail...</td>\n",
       "      <td>White Horse Ceramic Singapore is a leading man...</td>\n",
       "      <td>Tile Store</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Wealth Solution Partners</td>\n",
       "      <td>Super and SMSF Services | Financial Planning a...</td>\n",
       "      <td>WSP, Wealth Solution Partners, Financial Plann...</td>\n",
       "      <td>Wealth Solution Partners Pty Ltd is an indepen...</td>\n",
       "      <td>Investment Consultants &amp; Financial Advisors</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PMG</td>\n",
       "      <td>Fire and Water Cleanup Services | Mold Remedia...</td>\n",
       "      <td>PMG General Solutions Inc. is an environmental...</td>\n",
       "      <td>PMG General Solutions Inc. is an environmental...</td>\n",
       "      <td>Damage Restoration &amp; Mold Remediation</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TMP Capital PLLC</td>\n",
       "      <td>Licensed in AL &amp; FL | 203K Loans | 15-year Fix...</td>\n",
       "      <td>TMP Capital PLLC Consulting Company Franklin M...</td>\n",
       "      <td>TMP Capital PLLC Consulting Company, also know...</td>\n",
       "      <td>Mortgage Brokers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Genertek Power</td>\n",
       "      <td>Industrial and Commercial Energy Storage | Ass...</td>\n",
       "      <td>Genertek Power Ltd a UK electricity systems &amp; ...</td>\n",
       "      <td>Genertek Power Limited is a privately-owned UK...</td>\n",
       "      <td>Renewable energy companies</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626241</th>\n",
       "      <td>Global Golf Tech Solutions</td>\n",
       "      <td>Manufacturing | Golf Academy | G Launch Monito...</td>\n",
       "      <td>The best personal golf launch monitor screen p...</td>\n",
       "      <td>Global Golf Tech Solutions is a company that s...</td>\n",
       "      <td>Golf Courses &amp; Country Clubs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626242</th>\n",
       "      <td>Renko</td>\n",
       "      <td>Latest Processing Technologies | E Gaskets for...</td>\n",
       "      <td>EPDM Rubber products from Renko can be found o...</td>\n",
       "      <td>RENKO is a company that has been producing hig...</td>\n",
       "      <td>Fabricated Rubber Products</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626243</th>\n",
       "      <td>Norstal</td>\n",
       "      <td>Residential Buildings | Custom Project Service...</td>\n",
       "      <td>Norstal produces a broad range of steel struct...</td>\n",
       "      <td>Norstal is a steel structure producer that spe...</td>\n",
       "      <td>Metal Fabrication Services</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626244</th>\n",
       "      <td>Acoustic</td>\n",
       "      <td>Wood-based Acoustic Products Manufacturer | De...</td>\n",
       "      <td>We are designed and manufactured in UAE, Acous...</td>\n",
       "      <td>Acoustic.ae is a member of a UAE-based group o...</td>\n",
       "      <td>Building Material Manufacturers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>626245</th>\n",
       "      <td>PAPER MACHINE CLOTHING</td>\n",
       "      <td>Detailed Machine Studies and Analytics | Scree...</td>\n",
       "      <td>With Huatao innovative product portfolio of pa...</td>\n",
       "      <td>Huatao Group, also known as Huatao Group, is a...</td>\n",
       "      <td>Paper &amp; Cardboard Products</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>626246 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   commercial_name  \\\n",
       "0                      White Horse   \n",
       "1         Wealth Solution Partners   \n",
       "2                              PMG   \n",
       "3                 TMP Capital PLLC   \n",
       "4                   Genertek Power   \n",
       "...                            ...   \n",
       "626241  Global Golf Tech Solutions   \n",
       "626242                       Renko   \n",
       "626243                     Norstal   \n",
       "626244                    Acoustic   \n",
       "626245      PAPER MACHINE CLOTHING   \n",
       "\n",
       "                                            business_tags  \\\n",
       "0       Tile Manufacturing | European Aesthetics Ceram...   \n",
       "1       Super and SMSF Services | Financial Planning a...   \n",
       "2       Fire and Water Cleanup Services | Mold Remedia...   \n",
       "3       Licensed in AL & FL | 203K Loans | 15-year Fix...   \n",
       "4       Industrial and Commercial Energy Storage | Ass...   \n",
       "...                                                   ...   \n",
       "626241  Manufacturing | Golf Academy | G Launch Monito...   \n",
       "626242  Latest Processing Technologies | E Gaskets for...   \n",
       "626243  Residential Buildings | Custom Project Service...   \n",
       "626244  Wood-based Acoustic Products Manufacturer | De...   \n",
       "626245  Detailed Machine Studies and Analytics | Scree...   \n",
       "\n",
       "                                        short_description  \\\n",
       "0       White Horse is highly regarded as a tile trail...   \n",
       "1       WSP, Wealth Solution Partners, Financial Plann...   \n",
       "2       PMG General Solutions Inc. is an environmental...   \n",
       "3       TMP Capital PLLC Consulting Company Franklin M...   \n",
       "4       Genertek Power Ltd a UK electricity systems & ...   \n",
       "...                                                   ...   \n",
       "626241  The best personal golf launch monitor screen p...   \n",
       "626242  EPDM Rubber products from Renko can be found o...   \n",
       "626243  Norstal produces a broad range of steel struct...   \n",
       "626244  We are designed and manufactured in UAE, Acous...   \n",
       "626245  With Huatao innovative product portfolio of pa...   \n",
       "\n",
       "                                              description  \\\n",
       "0       White Horse Ceramic Singapore is a leading man...   \n",
       "1       Wealth Solution Partners Pty Ltd is an indepen...   \n",
       "2       PMG General Solutions Inc. is an environmental...   \n",
       "3       TMP Capital PLLC Consulting Company, also know...   \n",
       "4       Genertek Power Limited is a privately-owned UK...   \n",
       "...                                                   ...   \n",
       "626241  Global Golf Tech Solutions is a company that s...   \n",
       "626242  RENKO is a company that has been producing hig...   \n",
       "626243  Norstal is a steel structure producer that spe...   \n",
       "626244  Acoustic.ae is a member of a UAE-based group o...   \n",
       "626245  Huatao Group, also known as Huatao Group, is a...   \n",
       "\n",
       "                             main_business_category  \n",
       "0                                        Tile Store  \n",
       "1       Investment Consultants & Financial Advisors  \n",
       "2             Damage Restoration & Mold Remediation  \n",
       "3                                  Mortgage Brokers  \n",
       "4                        Renewable energy companies  \n",
       "...                                             ...  \n",
       "626241                 Golf Courses & Country Clubs  \n",
       "626242                   Fabricated Rubber Products  \n",
       "626243                   Metal Fabrication Services  \n",
       "626244              Building Material Manufacturers  \n",
       "626245                   Paper & Cardboard Products  \n",
       "\n",
       "[626246 rows x 5 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_parquet(DATA_TOURNAMENT)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business_tags', 'description', 'commercial_name', 'short_description'}\n"
     ]
    }
   ],
   "source": [
    "columns_round = set(df.columns)\n",
    "columns_round.remove(\"main_business_category\")\n",
    "print(columns_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Creating an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fitting the encoder\n",
    "encoder.fit(df[\"main_business_category\"].unique())\n",
    "\n",
    "# Transforming the data\n",
    "df[\"target\"] = encoder.transform(df[\"main_business_category\"])\n",
    "# print(\"Encoded data:\", encoded_data)\n",
    "\n",
    "# Inverse transforming the data\n",
    "# decoded_data = encoder.inverse_transform(encoded_data)\n",
    "# print(\"Decoded data:\", decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From a total of 569 it remains 551 classes. So 18 was deleted\n"
     ]
    }
   ],
   "source": [
    "def remove_rare_classes(df, target_column, more_than:int = 1):\n",
    "    \"\"\"Remove rows where the target class has only one occurrence.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The dataset to be filtered.\n",
    "        target_column (str): The column name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered dataset without rare classes.\n",
    "    \"\"\"\n",
    "    # Calculate the count of each class in the target column\n",
    "    value_counts = df[target_column].value_counts()\n",
    "\n",
    "    # Identify classes where the count is more than 1\n",
    "    classes_to_keep = value_counts[value_counts > more_than].index\n",
    "\n",
    "    # Filter the DataFrame to keep only rows with classes that have more than one occurrence\n",
    "    filtered_df = df[df[target_column].isin(classes_to_keep)]\n",
    "\n",
    "    print(\n",
    "        f\"From a total of {len(df[target_column].unique())} it remains {len(filtered_df[target_column].unique())} classes. So {len(df[target_column].unique()) - len(filtered_df[target_column].unique())} was deleted\"\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "df_filtered = remove_rare_classes(df, 'target',5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 500955, Validation size: 62619, Test size: 62620\n"
     ]
    }
   ],
   "source": [
    "df_train, df_val, df_test = split_dataset(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered[\"clean_text\"] = df_filtered[\"Tweet\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_bow(X_train, X_test):\n",
    "    # count_vect = CountVectorizer()\n",
    "\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "    tfidf_transformer = TfidfVectorizer()\n",
    "\n",
    "    # X_train = count_vect.fit_transform(X_train)\n",
    "    X_train = tfidf_transformer.fit_transform(X_train)\n",
    "\n",
    "    # X_test = count_vect.transform(X_test)\n",
    "    X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def apply_traditional_algorithms(df_train, df_test, pipeline_cls, threshold=0.5):\n",
    "\n",
    "    X_train, y_train = df_train[\"clean_text\"], df_train[\"target\"]\n",
    "    X_test, y_test = df_test[\"clean_text\"], df_test[\"target\"]\n",
    "\n",
    "    X_train, X_test = apply_bow(X_train, X_test)\n",
    "\n",
    "    best_model, best_model_name = None, None\n",
    "    base_f1_score = 0\n",
    "    for Name, cls in pipeline_cls.items():\n",
    "        cls.fit(X_train, y_train)\n",
    "        y_proba = cls.predict_proba(X_test)\n",
    "        y_pred = (y_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "        if f1_score(y_test, y_pred, average=\"macro\") > base_f1_score:\n",
    "            base_f1_score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            best_model = cls\n",
    "            best_model_name = Name\n",
    "\n",
    "\n",
    "        print(Name)\n",
    "        print(cls)\n",
    "        print_metrics(y_pred, y_test, Name + \" - Text\")\n",
    "        \n",
    "        print(f\"Test set F1-score: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "        print(f\"Test set Precision {precision_score(y_test,y_pred,average='macro')}\")\n",
    "        print(f\"Test set Recall {recall_score(y_test, y_pred,average='macro')}\")\n",
    "        print(\"\\n\\n\")\n",
    "        print(\n",
    "            \"-----------------------------------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "    return best_model, best_model_name, base_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_classifiers = {\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(class_weight=\"balanced\"),\n",
    "    \"XGBClassifier\": XGBClassifier(scale_pos_weight=1, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"SVC\": SVC(class_weight=\"balanced\", probability=True),\n",
    "    \"Logistic Regression\": LogisticRegression(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'business_tags', 'description', 'commercial_name', 'short_description'}\n"
     ]
    }
   ],
   "source": [
    "print(columns_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####################################################################\n",
      "######################## Round 0 #################################\n",
      "######################## business_tags #################################\n"
     ]
    }
   ],
   "source": [
    "for i, col_name in enumerate(columns_round):\n",
    "    print(\"#####################################################################\")\n",
    "    print(f\"######################## Round {i} #################################\")\n",
    "    print(f\"######################## {col_name} #################################\")\n",
    "\n",
    "    df_tmp_train, df_tmp_test = (\n",
    "        df_val[[col_name, \"target\"]],\n",
    "        df_test[[col_name, \"target\"]],\n",
    "    )\n",
    "    # df_tmp_train.rename(columns={col_name: \"clean_text\"}, inplace=True)\n",
    "    # df_tmp_test.rename(columns={col_name:\"clean_text\"},inplace=True)\n",
    "\n",
    "    df_tmp_train[\"clean_text\"] = df_filtered[col_name].apply(lambda x: clean_text(x))\n",
    "    df_tmp_test[\"clean_text\"] = df_filtered[col_name].apply(lambda x: clean_text(x))\n",
    "\n",
    "    apply_traditional_algorithms(df_tmp_train, df_tmp_test, pipeline_classifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "class BERTClass(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model: str = \"bert-base-uncased\",\n",
    "        num_classes: int = 2,\n",
    "        droput_rate: int = 0.3,\n",
    "    ):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model, return_dict=True)\n",
    "        self.dropout = nn.Dropout(droput_rate)\n",
    "        self.linear = nn.Linear(768, num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "\n",
    "        return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
