{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run if you run on colab\n",
    "# %pip install transformers\n",
    "# %pip install spacy\n",
    "# !python -m spacy download en_core_web_sm\n",
    "# !python -m spacy download en_core_web_lg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, BertModel\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Algorithms\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    ")\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy, re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_mentions_and_hashtags(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Remove punctuation using the translation table\n",
    "    text_without_punct = text.translate(translator)\n",
    "\n",
    "    return text_without_punct\n",
    "\n",
    "\n",
    "def remove_stopwords(text, to_lemmatize:bool = False):\n",
    "    filtered_sentence = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False:\n",
    "            if to_lemmatize:\n",
    "                filtered_sentence.append(token.lemma_)\n",
    "            else:\n",
    "                filtered_sentence.append(token.text)\n",
    "\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "\n",
    "def clean_text(text,to_lemmatize:bool = True):\n",
    "\n",
    "    # Standardize text\n",
    "    # text = standardize_accented_chars(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = remove_mentions_and_hashtags(text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = remove_numbers(text)\n",
    "\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text,to_lemmatize)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "    # if to_lemmatize:\n",
    "    #     text = lemmatize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metrics(y_pred, y_test, title: str = \"Confusion Matrix\"):\n",
    "    from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "    print(f\"Reports for {title}\")\n",
    "    print(f\"Balanced Accuracy: {balanced_accuracy_score(y_test, y_pred)}\")\n",
    "    print(f\"Precision: {precision_score(y_test, y_pred,average='weighted')}\")\n",
    "    print(f\"Recall: {recall_score(y_test, y_pred,average='weighted')}\")\n",
    "    print(f\"F1: {f1_score(y_test, y_pred,average='weighted')}\")\n",
    "    print(f\"Metthew corr: {matthews_corrcoef(y_test, y_pred)}\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # cm = confusion_matrix(y_test, y_pred)\n",
    "    # sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "    # plt.title(title)\n",
    "    # plt.xlabel(\"Predicted\")\n",
    "    # plt.ylabel(\"True\")\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(data):\n",
    "    \"\"\"Function to split dataset into train, val and test\"\"\"\n",
    "    # np.random.seed(112)\n",
    "    # df_train, df_val, df_test = np.split(\n",
    "    #     data.sample(frac=1, random_state=42),\n",
    "    #     [int(0.8 * len(data)), int(0.9 * len(data))],\n",
    "    # )\n",
    "\n",
    "    # # Print dimensions\n",
    "    # print(len(df_train), len(df_val), len(df_test))\n",
    "\n",
    "    # Split data into train (80%) and temp_test (20%)\n",
    "    df_train, temp_test = train_test_split(\n",
    "        data, test_size=0.1, random_state=42, stratify=data[\"target\"]\n",
    "    )\n",
    "\n",
    "    # Split temp_test into validation (50% of temp_test, 10% of total) and test (50% of temp_test, 10% of total)\n",
    "    df_val, df_test = train_test_split(\n",
    "        temp_test, test_size=0.5, random_state=42, stratify=temp_test[\"target\"]\n",
    "    )\n",
    "\n",
    "    # Print dimensions to verify\n",
    "    print(\n",
    "        f\"Train size: {len(df_train)}, Validation size: {len(df_val)}, Test size: {len(df_test)}\"\n",
    "    )\n",
    "\n",
    "    return df_train, df_val, df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Traditional algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# current_directory = os.getcwd()\n",
    "\n",
    "# parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "# DATA_PATH = os.path.join(parent_directory, \"data\")\n",
    "# DATA_TOURNAMENT = os.path.join(DATA_PATH, \"tournament_hints_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_directory = os.getcwd()\n",
    "\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "\n",
    "DATA_PATH = os.path.join(parent_directory,\"data\")\n",
    "DATA_TOURNAMENT = os.path.join(DATA_PATH,\"tournament_hints_data.parquet\")\n",
    "\n",
    "DATA_TOURNAMENT = os.path.join(DATA_PATH, \"tournament_hints_data.parquet\")\n",
    "\n",
    "\n",
    "print(\"Current Directory:\", current_directory)\n",
    "print(\"Parent Directory:\", parent_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(DATA_TOURNAMENT)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_round = set(df.columns)\n",
    "columns_round.remove(\"main_business_category\")\n",
    "print(columns_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Creating an instance of LabelEncoder\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "# Fitting the encoder\n",
    "encoder.fit(df[\"main_business_category\"].unique())\n",
    "\n",
    "# Transforming the data\n",
    "df[\"target\"] = encoder.transform(df[\"main_business_category\"])\n",
    "# print(\"Encoded data:\", encoded_data)\n",
    "\n",
    "# Inverse transforming the data\n",
    "# decoded_data = encoder.inverse_transform(encoded_data)\n",
    "# print(\"Decoded data:\", decoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_classes(df, target_column, more_than:int = 1):\n",
    "    \"\"\"Remove rows where the target class has only one occurrence.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The dataset to be filtered.\n",
    "        target_column (str): The column name of the target variable.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame: Filtered dataset without rare classes.\n",
    "    \"\"\"\n",
    "    # Calculate the count of each class in the target column\n",
    "    value_counts = df[target_column].value_counts()\n",
    "\n",
    "    # Identify classes where the count is more than 1\n",
    "    classes_to_keep = value_counts[value_counts > more_than].index\n",
    "\n",
    "    # Filter the DataFrame to keep only rows with classes that have more than one occurrence\n",
    "    filtered_df = df[df[target_column].isin(classes_to_keep)]\n",
    "\n",
    "\n",
    "    print(\n",
    "        f\"From a total of {len(df[target_column].unique())} it remains {len(filtered_df[target_column].unique())} classes. So {len(df[target_column].unique()) - len(filtered_df[target_column].unique())} was deleted\"\n",
    "    )\n",
    "\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "df_filtered = remove_rare_classes(df, 'target',15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_val, df_test = split_dataset(df_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_filtered[\"clean_text\"] = df_filtered[\"Tweet\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def apply_bow(X_train, X_test):\n",
    "    # count_vect = CountVectorizer()\n",
    "\n",
    "    print(\"Apply tfidf\")\n",
    "    tfidf_transformer = TfidfVectorizer()\n",
    "\n",
    "    # X_train = count_vect.fit_transform(X_train)\n",
    "    X_train = tfidf_transformer.fit_transform(X_train)\n",
    "\n",
    "    # X_test = count_vect.transform(X_test)\n",
    "    X_test = tfidf_transformer.transform(X_test)\n",
    "\n",
    "    return X_train, X_test\n",
    "\n",
    "\n",
    "def apply_traditional_algorithms(df_train, df_test, pipeline_cls, threshold=0.5):\n",
    "\n",
    "    X_train, y_train = df_train[\"clean_text\"], df_train[\"target\"]\n",
    "    X_test, y_test = df_test[\"clean_text\"], df_test[\"target\"]\n",
    "\n",
    "    X_train, X_test = apply_bow(X_train, X_test)\n",
    "\n",
    "    best_model, best_model_name = None, None\n",
    "    base_f1_score = 0\n",
    "    \n",
    "    for Name, cls in pipeline_cls.items():\n",
    "        print(f\"Start training {Name}\")\n",
    "        cls.fit(X_train, y_train)\n",
    "        y_proba = cls.predict_proba(X_test)\n",
    "        y_pred = (y_proba[:, 1] >= threshold).astype(int)\n",
    "\n",
    "        if f1_score(y_test, y_pred, average=\"macro\") > base_f1_score:\n",
    "            base_f1_score = f1_score(y_test, y_pred, average=\"macro\")\n",
    "            best_model = cls\n",
    "            best_model_name = Name\n",
    "\n",
    "        print_metrics(y_pred, y_test, Name + \" - Text\")\n",
    "        \n",
    "        print(f\"Test set F1-score: {f1_score(y_test, y_pred, average='macro')}\")\n",
    "        print(f\"Test set Precision {precision_score(y_test,y_pred,average='macro')}\")\n",
    "        print(f\"Test set Recall {recall_score(y_test, y_pred,average='macro')}\")\n",
    "        print(\"\\n\\n\")\n",
    "        print(\n",
    "            \"-----------------------------------------------------------------------------------------------------------------------\"\n",
    "        )\n",
    "\n",
    "    return best_model, best_model_name, base_f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_classifiers = {\n",
    "    # \"XGBClassifier\": XGBClassifier(scale_pos_weight=1, use_label_encoder=False, eval_metric='mlogloss'),\n",
    "    \"RandomForestClassifier\": RandomForestClassifier(class_weight=\"balanced\"),\n",
    "    \"SVC\": SVC(class_weight=\"balanced\", probability=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(columns_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col_name in enumerate(columns_round):\n",
    "    print(\"######################################################################\")\n",
    "    print(f\"######################## Round {i} ##################################\")\n",
    "    print(f\"######################## {col_name} #################################\")\n",
    "\n",
    "    df_tmp_train, df_tmp_test = (\n",
    "        df_val[[col_name, \"target\"]],\n",
    "        df_test[[col_name, \"target\"]],\n",
    "    )\n",
    "    # df_tmp_train.rename(columns={col_name: \"clean_text\"}, inplace=True)\n",
    "    # df_tmp_test.rename(columns={col_name:\"clean_text\"},inplace=True)\n",
    "\n",
    "    df_tmp_train[\"clean_text\"] = df_tmp_train[col_name].apply(\n",
    "        lambda x: clean_text(x, to_lemmatize=False)\n",
    "    )\n",
    "    df_tmp_test[\"clean_text\"] = df_tmp_test[col_name].apply(\n",
    "        lambda x: clean_text(x, to_lemmatize=False)\n",
    "    )\n",
    "\n",
    "    apply_traditional_algorithms(df_tmp_train, df_tmp_test, pipeline_classifiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "\n",
    "class BERTClass(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        bert_model: str = \"bert-base-uncased\",\n",
    "        num_classes: int = 2,\n",
    "        droput_rate: int = 0.3,\n",
    "    ):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.bert_model = BertModel.from_pretrained(bert_model, return_dict=True)\n",
    "        self.dropout = nn.Dropout(droput_rate)\n",
    "        self.linear = nn.Linear(768, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        output = self.bert_model(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "        output_dropout = self.dropout(output.pooler_output)\n",
    "        output = self.linear(output_dropout)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data:pd.DataFrame,tokenizer,max_length:int=128):\n",
    "\n",
    "        self.labels = data[\"target\"].to_list()\n",
    "        self.max_length = max_length\n",
    "        self.texts = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.max_length,\n",
    "                truncation=True,\n",
    "                return_tensors=\"pt\",\n",
    "            )\n",
    "            for text in df[\"text\"]\n",
    "        ]\n",
    "\n",
    "    def get_max_len(self, df):\n",
    "        return max(self.texts)\n",
    "\n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train\n",
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=16, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=16, shuffle=False)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "        criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "        total_acc_train = 0\n",
    "        total_loss_train = 0\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "            train_label = train_label.to(device)\n",
    "            mask = train_input[\"attention_mask\"].to(device)\n",
    "            input_id = train_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            batch_loss = criterion(output, train_label.long())\n",
    "            total_loss_train += batch_loss.item()\n",
    "\n",
    "            acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "            total_acc_train += acc\n",
    "\n",
    "            model.zero_grad()\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        total_acc_val = 0\n",
    "        total_loss_val = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for val_input, val_label in val_dataloader:\n",
    "\n",
    "                val_label = val_label.to(device)\n",
    "                mask = val_input[\"attention_mask\"].to(device)\n",
    "                input_id = val_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "\n",
    "                batch_loss = criterion(output, val_label.long())\n",
    "                total_loss_val += batch_loss.item()\n",
    "\n",
    "                acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                total_acc_val += acc\n",
    "\n",
    "        print(\n",
    "            f\"Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "                | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "                | Val Accuracy: {total_acc_val / len(val_data): .3f}\"\n",
    "        )\n",
    "\n",
    "# Evaluate\n",
    "\n",
    "\n",
    "def evaluate(model, test_data, model_name: str = \"Model name\"):\n",
    "\n",
    "    test = Dataset(test_data)\n",
    "\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "        model = model.cuda()\n",
    "\n",
    "    total_acc_test = 0\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input[\"attention_mask\"].to(device)\n",
    "            input_id = test_input[\"input_ids\"].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "\n",
    "            # predictions = output.argmax(dim=1)\n",
    "\n",
    "            prediction = F.softmax(output, dim=1).detach().cpu().numpy()\n",
    "            threshold = 0.3\n",
    "            preds = np.where(prediction[:, 1] > threshold, 1, 0)\n",
    "\n",
    "            acc = (output.argmax(dim=1) == test_label).sum().item()\n",
    "\n",
    "            total_acc_test += acc\n",
    "\n",
    "            y_true.extend(test_label.cpu().numpy().tolist())\n",
    "            y_pred.extend(preds)\n",
    "\n",
    "    print(classification_report(y_true, y_pred))\n",
    "\n",
    "    # plot confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    sns.heatmap(cm, annot=True, fmt=\"g\")\n",
    "    plt.title(model_name)\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.show()\n",
    "\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"Test Accuracy: {total_acc_test / len(test_data): .3f}\")\n",
    "    print(f\"Test Precison: {precision: .3f}\")\n",
    "    print(f\"Test Recall: {recall: .3f}\")\n",
    "    print(f\"Test F1: {f1: .3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
