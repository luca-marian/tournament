{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULCRUM - TOURNAMENT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# alex\n",
    "dataset_path = ''\n",
    "\n",
    "embeddings_commercial_name_path = './data/'\n",
    "embeddings_short_description_path = './data/'\n",
    "embeddings_description_path = './data/'\n",
    "\n",
    "# vali \n",
    "NAICS_KEYWORDS_PATH = './data/naics_summary_keywords.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19abb36245a34cb0bc795532b780465f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\valen\\AppData\\Roaming\\Python\\Python38\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\valen\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f46fdda6cd4c01817ad1697eaeca77",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb7e2ccfdf24f589fa2d8ac683e7666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f73362994d344f6a261633dcb8d24ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4152edcee75f42c6b08615deacb32fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dc0d8ddac8c4900891a25de75122c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12870560ea64466f920bcc97639b0896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a97a7572bd614066bd463a51e3766073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9832fef110049739d62c87524528583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2650545b53b4cf28c7912df395dc52a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "709ca06853224fada5041e74e143ce7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: ''",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m embeddings_short_description_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m embeddings_description_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 11\u001b[0m loaded_embeddings_commercial_name \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43membeddings_commercial_name_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m loaded_embeddings_short_description \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(embeddings_short_description_path)\n\u001b[0;32m     13\u001b[0m loaded_embeddings_description \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(embeddings_description_path)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\numpy\\lib\\npyio.py:405\u001b[0m, in \u001b[0;36mload\u001b[1;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[0;32m    403\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 405\u001b[0m     fid \u001b[38;5;241m=\u001b[39m stack\u001b[38;5;241m.\u001b[39menter_context(\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mos_fspath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    406\u001b[0m     own_fid \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;66;03m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: ''"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('all-mpnet-base-v2')\n",
    "\n",
    "\n",
    "loaded_embeddings_commercial_name = np.load(embeddings_commercial_name_path)\n",
    "loaded_embeddings_short_description = np.load(embeddings_short_description_path)\n",
    "loaded_embeddings_description = np.load(embeddings_description_path)\n",
    "\n",
    "data = pd.DataFrame(pd.read_csv(dataset_path))\n",
    "\n",
    "#! Commercial name - cred\n",
    "\n",
    "def round_1(company_name):\n",
    "    sen = [company_name]\n",
    "    sen_embeddings = model.encode(sen)\n",
    "\n",
    "    results = cosine_similarity(\n",
    "        [sen_embeddings[0]],\n",
    "        loaded_embeddings_commercial_name[0:]\n",
    "    )\n",
    "    sim_value = results.max() # value\n",
    "    label = data.iloc[results.argmax()]['naics_label'] # label\n",
    "    return label\n",
    "\n",
    "#! Short description\n",
    "\n",
    "def round_3(short_description):\n",
    "    sen = [short_description]\n",
    "    sen_embeddings = model.encode(sen)\n",
    "\n",
    "    results = cosine_similarity(\n",
    "        [sen_embeddings[0]],\n",
    "        loaded_embeddings_short_description[0:]\n",
    "    )\n",
    "    sim_value = results.max()\n",
    "    label = data.iloc[results.argmax()]['naics_label']\n",
    "    return label\n",
    "\n",
    "#! Description\n",
    "\n",
    "def round_5(description):\n",
    "    sen = [description]\n",
    "    sen_embeddings = model.encode(sen)\n",
    "\n",
    "    results = cosine_similarity(\n",
    "        [sen_embeddings[0]],\n",
    "        loaded_embeddings_description[0:]\n",
    "    )\n",
    "    sim_value = results.max()\n",
    "    label = data.iloc[results.argmax()]['naics_label']\n",
    "    return label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Vali - keywords for Round 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>naics_code</th>\n",
       "      <th>naics_label</th>\n",
       "      <th>description</th>\n",
       "      <th>summary</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>111</td>\n",
       "      <td>Crop Production</td>\n",
       "      <td>Industries in the Crop Production subsector gr...</td>\n",
       "      <td>The Crop Production subsector comprises establ...</td>\n",
       "      <td>Crop Production, farms, orchards, greenhouses,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>112</td>\n",
       "      <td>Animal Production and Aquaculture</td>\n",
       "      <td>Industries in the Animal Production and Aquacu...</td>\n",
       "      <td>The Animal Production and Aquaculture subsecto...</td>\n",
       "      <td>Animal Production, Aquaculture, Ranches, Farms...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>113</td>\n",
       "      <td>Forestry and Logging</td>\n",
       "      <td>Industries in the Forestry and Logging subsect...</td>\n",
       "      <td>The forestry and logging industries harvest ti...</td>\n",
       "      <td>Forestry, Logging, Timber Production, Reforest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>114</td>\n",
       "      <td>Fishing, Hunting and Trapping</td>\n",
       "      <td>Industries in the Fishing, Hunting and Trappin...</td>\n",
       "      <td>The Fishing, Hunting and Trapping industry rel...</td>\n",
       "      <td>Fishing, Hunting, Trapping, Harvest, Wild Anim...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>115</td>\n",
       "      <td>Support Activities for Agriculture and Forestry</td>\n",
       "      <td>Industries in the Support Activities for Agric...</td>\n",
       "      <td>The Support Activities for Agriculture and For...</td>\n",
       "      <td>Support Activities, Agriculture, Forestry, Sup...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  naics_code                                      naics_label  \\\n",
       "0           0         111                                  Crop Production   \n",
       "1           1         112                Animal Production and Aquaculture   \n",
       "2           2         113                             Forestry and Logging   \n",
       "3           3         114                    Fishing, Hunting and Trapping   \n",
       "4           4         115  Support Activities for Agriculture and Forestry   \n",
       "\n",
       "                                         description  \\\n",
       "0  Industries in the Crop Production subsector gr...   \n",
       "1  Industries in the Animal Production and Aquacu...   \n",
       "2  Industries in the Forestry and Logging subsect...   \n",
       "3  Industries in the Fishing, Hunting and Trappin...   \n",
       "4  Industries in the Support Activities for Agric...   \n",
       "\n",
       "                                             summary  \\\n",
       "0  The Crop Production subsector comprises establ...   \n",
       "1  The Animal Production and Aquaculture subsecto...   \n",
       "2  The forestry and logging industries harvest ti...   \n",
       "3  The Fishing, Hunting and Trapping industry rel...   \n",
       "4  The Support Activities for Agriculture and For...   \n",
       "\n",
       "                                            keywords  \n",
       "0  Crop Production, farms, orchards, greenhouses,...  \n",
       "1  Animal Production, Aquaculture, Ranches, Farms...  \n",
       "2  Forestry, Logging, Timber Production, Reforest...  \n",
       "3  Fishing, Hunting, Trapping, Harvest, Wild Anim...  \n",
       "4  Support Activities, Agriculture, Forestry, Sup...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_naics_keyw = pd.read_csv(NAICS_KEYWORDS_PATH)\n",
    "pd_naics_keyw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "\n",
    "def remove_mentions_and_hashtags(text):\n",
    "    text = re.sub(r\"@\\w+\", \"\", text)\n",
    "    text = re.sub(r\"#\\w+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_numbers(text):\n",
    "    text = re.sub(r\"\\d+\", \"\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "\n",
    "    # Remove punctuation using the translation table\n",
    "    text_without_punct = text.translate(translator)\n",
    "\n",
    "    return text_without_punct\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    filtered_sentence = []\n",
    "    doc = nlp(text)\n",
    "    for token in doc:\n",
    "        if token.is_stop == False:\n",
    "            filtered_sentence.append(token.text)\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "\n",
    "def clean_text(text,to_lemmatize:bool = True):\n",
    "\n",
    "    # Standardize text\n",
    "    # text = standardize_accented_chars(text)\n",
    "\n",
    "    # Remove URLs\n",
    "    text = re.sub(r\"http\\S+\", \"\", text)\n",
    "\n",
    "    # Remove mentions and hashtags\n",
    "    text = remove_mentions_and_hashtags(text)\n",
    "\n",
    "    # Lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = remove_punctuation(text)\n",
    "\n",
    "    # Remove numbers\n",
    "    text = remove_numbers(text)\n",
    "\n",
    "    # Remove all the special characters\n",
    "    text = re.sub(r\"\\W\", \" \", text)\n",
    "\n",
    "    # Remove stopwords\n",
    "    text = remove_stopwords(text)\n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    text = re.sub(r\"\\s+\", \" \", text, flags=re.I)\n",
    "\n",
    "    if to_lemmatize:\n",
    "        text = lemmatize(text)\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def lemmatize(text):\n",
    "    doc = nlp(text)\n",
    "    text = \" \".join([token.lemma_ for token in doc])\n",
    "    return text\n",
    "\n",
    "# Calculate similarity\n",
    "def calculate_similarity(keywords, tokens):\n",
    "    \n",
    "    text1 = nlp(\" \".join(keywords))\n",
    "    text2 = nlp(\" \".join(tokens))\n",
    "    \n",
    "    return text1.similarity(text2)\n",
    "\n",
    "\n",
    "def predict_industry_label(keywords, industry_data):\n",
    "    label_key = 'naics_label'\n",
    "    code_key = 'naics_code'\n",
    "\n",
    "#Precompute similarities if possible\n",
    "    similarities = industry_data['tokens'].apply(lambda tokens: calculate_similarity(keywords, tokens))\n",
    "\n",
    "    # Find index of the maximum similarity\n",
    "    max_index = similarities.idxmax()\n",
    "    max_similarity = similarities[max_index]\n",
    "    \n",
    "    # predicted_label = industry_data.at[max_index, code_key]\n",
    "    # predicted_label = str(predicted_label) + \" \" + str(industry_data.at[max_index, label_key])\n",
    "    predicted_code = str(industry_data.at[max_index, code_key])\n",
    "    predicted_label = str(industry_data.at[max_index, label_key])\n",
    "\n",
    "    return predicted_label, predicted_code, max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "textile product mills nonapparel textile product sheet towel cut sew textile industry textile product purchase fabric cutting sewing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "industry_data = pd_naics_keyw\n",
    "\n",
    "# Preprocess industry descriptions\n",
    "industry_data['processed_description'] = industry_data['keywords'].str.lower()\n",
    "industry_data['processed_description'] = [clean_text(text) for text in industry_data['processed_description']]\n",
    "\n",
    "\n",
    "print(industry_data['processed_description'][15])\n",
    "\n",
    "# apply clean_text function to the column 'processed_description'\n",
    "industry_data['processed_description'] = industry_data['processed_description'].apply(lambda x: clean_text(x))\n",
    "\n",
    "# Tokenize and embed industry descriptions\n",
    "industry_data['tokens'] = industry_data['processed_description'].apply(lambda x: [token.text for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200 {'company_id': 164, 'level': 1, 'hint': 'Happy Camper Wellness'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords:  ['Happy Camper Wellness']\n",
      "Guess:  Sporting Goods, Hobby, Musical Instrument, Book, and Miscellaneous Retailers\n",
      "Code:  459\n",
      "Similarity:  0.33569560249402364\n",
      "200 {'result': 'wrong', 'score': -50.0, 'answer': '624 Social Assistance'}\n",
      "\n",
      "200 {'company_id': 164, 'level': 2, 'hint': 'Counseling and Coaching Services | Total Body Wellness | Mobile Fitness and Wellness Services | Individual Therapy | Career Cultivation | Licensed Clinical Social Worker | Licensed in Louisiana | Mindfulness | Anxiety & Depression | ADHD Parenting | Mental Illnesses | Therapy to Go Services | Consulting Services | Womens Health'}\n",
      "Keywords:  ['Counseling', 'Coaching Services', 'Total Body Wellness']\n",
      "Guess:  Social Assistance\n",
      "Code:  624\n",
      "Similarity:  0.5726204726115647\n",
      "200 {'result': 'correct', 'score': 400.0, 'answer': '624 Social Assistance'}\n",
      "\n",
      "200 {'company_id': 164, 'level': 3, 'hint': 'Therapy, counseling, coaching using mindfulness techniques to improve happiness at Happy Camper Wellness in Metairie, LA'}\n",
      "Keywords:  ['Therapy', 'counseling', 'mindfulness techniques']\n",
      "Guess:  Professional, Scientific, and Technical Services\n",
      "Code:  541\n",
      "Similarity:  0.7016166371877394\n",
      "200 {'result': 'wrong', 'score': -150.0, 'answer': '624 Social Assistance'}\n",
      "\n",
      "200 {'company_id': 164, 'level': 4, 'hint': \"Happy Camper Wellness LLC is a mobile wellness company based in Metairie, LA that offers therapy, counseling, coaching, consulting, and therapy services to individuals, companies, and the community. The company provides sessions in a variety of unique settings intended to stimulate the mind and body in a positive way, including Drop a Pin, Brick 'n Mortar, Telesessions, Walk&Talk, and Camper Counseling. The founder, Michelle Swanford, is a licensed Clinical Therapist and Life Coach with over 20 years of experience who is passionate about boosting happiness in all those she serves. She has worked with those needing help navigating career and relationship challenges, as well as those needing therapeutic interventions for severe mental illnesses, addictions, grief, depression/anxiety, and many more. The first session is 1 hour, with a comprehensive assessment of the client's mind/body and presenting condition.\"}\n",
      "Keywords:  ['Happy Camper Wellness LLC', 'a mobile wellness company', 'Metairie']\n",
      "Guess:  Furniture, Home Furnishings, Electronics, and Appliance Retailers\n",
      "Code:  449\n",
      "Similarity:  0.6849395379039362\n",
      "200 {'result': 'wrong', 'score': -200.0, 'answer': '624 Social Assistance'}\n",
      "\n",
      "200 {'company_id': 164, 'level': 5, 'hint': 'Psychologists & Counseling Services'}\n",
      "Keywords:  ['Psychologists', 'Counseling Services']\n",
      "Guess:  Hospitals\n",
      "Code:  622\n",
      "Similarity:  0.6814730505356216\n",
      "200 {'result': 'wrong', 'score': -250.0, 'answer': '624 Social Assistance'}\n",
      "\n",
      "200 {'response': 'Wait for 74.78571152687073 seconds'}\n"
     ]
    }
   ],
   "source": [
    "base_url = 'http://116.202.111.229:8000'\n",
    "api_key = 'VPLrVk4hSZMdGrW2wAP0GTpsV2Jsdx5Z'\n",
    "\n",
    "headers = {\n",
    "    'x-api-key': api_key\n",
    "}\n",
    "\n",
    "rounds = 5\n",
    "current_round = 1\n",
    "\n",
    "filename = 'extra_data.csv'\n",
    "data_columns = ['commercial_name', 'business_tags', 'short_descripti', 'description', 'main_business_category', 'naics']\n",
    "\n",
    "index = 0\n",
    "\n",
    "# Get a new hint for current company or get the first hint for a new company after calling /evaluate/reset\n",
    "while current_round != 6:\n",
    "    response = requests.get(f\"{base_url}/evaluate/hint\", headers=headers)\n",
    "\n",
    "    print(response.status_code, response.json())\n",
    "    \n",
    "    # Get the hint\n",
    "    hint = response.json()['hint']\n",
    "\n",
    "    # predict based off given hint\n",
    "    time.sleep(1)\n",
    "    \n",
    "    final_answer = \"abstain\"\n",
    "    \n",
    "    if current_round == 3 or 4:\n",
    "        # get the first 5 keywords from the text using spacy\n",
    "        doc = nlp(hint)\n",
    "        input_keywords = [chunk.text for chunk in doc.noun_chunks][:3]\n",
    "    else:\n",
    "        input_keywords = hint.split(\"|\")\n",
    "    \n",
    "    print(\"Keywords: \", input_keywords)\n",
    "    [final_answer, final_code, final_similarity] = predict_industry_label(input_keywords, industry_data)\n",
    "\n",
    "    # Post your answer for current hint\n",
    "    data = {\n",
    "        'answer': final_answer\n",
    "    }\n",
    "    \n",
    "    print(\"Guess: \", final_answer)\n",
    "    print(\"Code: \", final_code)\n",
    "    print(\"Similarity: \", final_similarity)\n",
    "    \n",
    "    response = requests.post(f\"{base_url}/evaluate/answer\", json=data, headers=headers)\n",
    "\n",
    "    print(response.status_code, response.json())\n",
    "    \n",
    "    print(\"\")\n",
    "    \n",
    "    current_round = current_round + 1\n",
    "    time.sleep(1)\n",
    "\n",
    "# Get hints about a new company\n",
    "current_round = 1\n",
    "index = index + 1\n",
    "\n",
    "response = requests.get(f\"{base_url}/evaluate/reset\", headers=headers)\n",
    "\n",
    "print(response.status_code, response.json())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
